# app/langgraph/nodes/policy_retriever_node.py
# -*- coding: utf-8 -*-
"""
policy_retriever_node.py (ë‹¨ìˆœí™”/ì¬ì„¤ê³„ + ì»¬ë ‰ì…˜ ê³„ì¸µ + synthetic query ë²„ì „)

ì—­í• :
  1) user_context_nodeê°€ ë§Œë“  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ ê²€ìƒ‰ìš© search_text êµ¬ì„±
     - profile_summary_text
     - merged_collection(ì§ˆí™˜/ì¹˜ë£Œ ìš”ì•½)
     - í˜„ì¬ ì§ˆë¬¸(user_input)
  2) "ì •ì±… ìš”ì²­ ë¬¸ì¥" ì„ë² ë”© + pgvector ê¸°ë°˜ ì •ì±… DB ê²€ìƒ‰ (ì œëª© titleë§Œ)
     - ë‹¨, ì§ˆë¬¸ì´ ë„ˆë¬´ ì¼ë°˜ì ì´ë©´ profile/collection ê¸°ë°˜ synthetic query ì‚¬ìš©
  3) region + í”„ë¡œí•„ ê¸°ë°˜ í•˜ë“œ í•„í„°ë§
  4) ì»¬ë ‰ì…˜ ê³„ì¸µ(L0/L1/L2) íŠ¸ë¦¬í”Œì„ BM25 í‚¤ì›Œë“œë¡œ ì‚¬ìš©
     - L0(ì´ë²ˆ í„´) > L1(ì´ë²ˆ ì„¸ì…˜) > L2(DB) ìˆœìœ¼ë¡œ step-weight ê°€ì¤‘
  5) hybrid score(ë²¡í„° + BM25)ë¡œ ìµœì¢… ë­í‚¹
  6) state["retrieval"], state["rag_snippets"], state["context"] ì„¸íŒ…
"""

from __future__ import annotations

import os
import re
import math
import hashlib
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime

import psycopg
from psycopg_pool import ConnectionPool
from dotenv import load_dotenv
from openai import OpenAI

# LangSmith trace ë°ì½”ë ˆì´í„° (ì—†ìœ¼ë©´ no-op)
try:
    from langsmith import traceable
except Exception:  # pragma: no cover

    def traceable(func):
        return func


from app.langgraph.state.ephemeral_context import State
from app.langgraph.utils.retrieval_filters import filter_candidates_by_profile

load_dotenv()

# -------------------------------------------------------------------
# DB URL
# -------------------------------------------------------------------
DB_URL = os.getenv("DATABASE_URL")
if not DB_URL:
    raise RuntimeError("DATABASE_URL not configured")

if DB_URL.startswith("postgresql+psycopg://"):
    DB_URL = DB_URL.replace("postgresql+psycopg://", "postgresql://", 1)

# -------------------------------------------------------------------
# Retriever tunable parameters
# -------------------------------------------------------------------
# âš¡ ì„±ëŠ¥/ë©”ëª¨ë¦¬ ìµœì í™”: ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ë¥¼ ì¤„ì—¬ì„œ ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶• + ë©”ëª¨ë¦¬ ì ˆì•½
RAW_TOP_K = int(os.getenv("POLICY_RETRIEVER_RAW_TOP_K", "8"))  # 12 â†’ 8 (1GB RAM ìµœì í™”)
CONTEXT_TOP_K = int(os.getenv("POLICY_RETRIEVER_CONTEXT_TOP_K", "5"))  # 8 â†’ 5 (1GB RAM ìµœì í™”)
SIMILARITY_FLOOR = float(os.getenv("POLICY_RETRIEVER_SIM_FLOOR", "0.3"))
MIN_CANDIDATES_AFTER_FLOOR = int(os.getenv("POLICY_RETRIEVER_MIN_AFTER_FLOOR", "5"))
BM25_WEIGHT = float(os.getenv("POLICY_RETRIEVER_BM25_WEIGHT", "0.2"))  # 0.35 â†’ 0.2

# ì»¬ë ‰ì…˜ ê³„ì¸µë³„ weight (L0 > L1 > L2)
LAYER_WEIGHTS = {
    "L0": 3,  # ì´ë²ˆ í„´ ìƒˆë¡œ ì¶”ì¶œëœ triples
    "L1": 2,  # ì´ë²ˆ ì„¸ì…˜ ephemeral_collection
    "L2": 1,  # DB collections
}


# -------------------------------------------------------------------
# OpenAI ì„ë² ë”© ì„¤ì •
# -------------------------------------------------------------------
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY not configured in .env")

OPENAI_MODEL = "text-embedding-3-small"  # 1536ì°¨ì›
EMBEDDING_CACHE_SIZE = 30  # ìµœëŒ€ ìºì‹œ ê°œìˆ˜ (ë©”ëª¨ë¦¬ ìµœì í™”)

# ì „ì—­ ìƒíƒœ
_openai_client: Optional[OpenAI] = None
_connection_pool: Optional[ConnectionPool] = None
_embedding_cache: Dict[str, List[float]] = {}  # cache_key -> ì„ë² ë”©
_cache_order: List[str] = []  # FIFO ë°©ì‹ ìºì‹œ ì œê±°ìš© í


def _get_openai_client() -> OpenAI:
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„± (ì‹±ê¸€í†¤)"""
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI(api_key=OPENAI_API_KEY)
        print("  âœ… [OpenAI] í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ", flush=True)
    return _openai_client


def _get_connection_pool() -> ConnectionPool:
    """DB ì—°ê²° í’€ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„± (ì‹±ê¸€í†¤)"""
    global _connection_pool
    if _connection_pool is None:
        _connection_pool = ConnectionPool(
            conninfo=DB_URL,
            min_size=1,  # ìµœì†Œ 1ê°œ ì—°ê²° ìœ ì§€ (ë©”ëª¨ë¦¬ ìµœì í™”)
            max_size=3,  # ìµœëŒ€ 3ê°œ ë™ì‹œ ì—°ê²° (ë©”ëª¨ë¦¬ ìµœì í™”)
            timeout=30,
            max_lifetime=300,  # 5ë¶„ë§ˆë‹¤ ì—°ê²° ì¬í™œìš© (stale connection ë°©ì§€)
            max_idle=60,  # 60ì´ˆ ìœ íœ´ ì—°ê²° ì¢…ë£Œ
            reconnect_timeout=10,  # ì¬ì—°ê²° íƒ€ì„ì•„ì›ƒ
        )
        print(
            "  âœ… [DB Pool] ì—°ê²° í’€ ì´ˆê¸°í™” ì™„ë£Œ (1-3ê°œ ì—°ê²°, 5ë¶„ ì¬í™œìš©)", flush=True
        )
    return _connection_pool


def _embed_text(text: str) -> List[float]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„± (ìºì‹± í¬í•¨)

    ì„±ëŠ¥:
    - ìºì‹œ íˆíŠ¸: ~0.001s (800ë°° ë¹ ë¦„)
    - ìºì‹œ ë¯¸ìŠ¤: ~0.8s (ê¸°ì¡´ 21s â†’ 27ë°° ë¹ ë¦„)

    Returns:
        1536ì°¨ì› ì„ë² ë”© ë²¡í„° (text-embedding-3-small)
    """
    import time

    text_to_embed = (text or "").strip()
    if not text_to_embed:
        # ë¹ˆ í…ìŠ¤íŠ¸: ì œë¡œ ë²¡í„° ë°˜í™˜
        return [0.0] * 1536

    # 1. ìºì‹œ í‚¤ ìƒì„±
    cache_start = time.time()
    cache_key = hashlib.md5(text_to_embed.encode("utf-8")).hexdigest()

    # 2. ìºì‹œ ì¡°íšŒ
    global _embedding_cache, _cache_order
    if cache_key in _embedding_cache:
        cache_elapsed = time.time() - cache_start
        print(
            f"  âœ… [ìºì‹œ HIT] {cache_elapsed:.4f}s, key: {cache_key[:8]}..., text_len: {len(text_to_embed)} chars",
            flush=True,
        )
        return _embedding_cache[cache_key]

    cache_elapsed = time.time() - cache_start
    print(f"  âš ï¸  [ìºì‹œ MISS] {cache_elapsed:.4f}s, OpenAI API í˜¸ì¶œ ì¤‘...", flush=True)

    # 3. OpenAI API í˜¸ì¶œ
    api_start = time.time()
    try:
        client = _get_openai_client()
        response = client.embeddings.create(model=OPENAI_MODEL, input=text_to_embed)
        embedding = response.data[0].embedding
        api_elapsed = time.time() - api_start
        print(
            f"  ğŸ” [OpenAI API] {api_elapsed:.2f}s, text_len: {len(text_to_embed)} chars",
            flush=True,
        )

        # 4. ìºì‹œ ì €ì¥ (FIFO ë°©ì‹)
        _embedding_cache[cache_key] = embedding
        _cache_order.append(cache_key)

        # ìºì‹œê°€ ê°€ë“ ì°¨ë©´ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        if len(_cache_order) > EMBEDDING_CACHE_SIZE:
            oldest_key = _cache_order.pop(0)
            _embedding_cache.pop(oldest_key, None)
            print(
                f"  ğŸ—‘ï¸  [ìºì‹œ] ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (ìºì‹œ í¬ê¸°: {len(_embedding_cache)})",
                flush=True,
            )

        return embedding

    except Exception as e:
        print(f"  âŒ [OpenAI API ì˜¤ë¥˜] {e}", flush=True)
        # ì˜¤ë¥˜ ì‹œ: ì œë¡œ ë²¡í„° ë°˜í™˜
        return [0.0] * 1536


# -------------------------------------------------------------------
# DB Connection
# -------------------------------------------------------------------
def _get_conn():
    return psycopg.connect(DB_URL)


# -------------------------------------------------------------------
# Keyword Extraction
# -------------------------------------------------------------------
def extract_keywords(text: str, max_k: int = 8) -> List[str]:
    """
    ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì—ì„œ í•œê¸€/ì˜ë¬¸/ìˆ«ì í† í°ë§Œ ë½‘ê³ 
    ìì£¼ ì“°ì´ëŠ” ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œ ë’¤ ìƒìœ„ max_kê°œë§Œ ë°˜í™˜.
    """
    if not text:
        return []
    tokens = re.findall(r"[ê°€-í£A-Za-z0-9]+", text)
    stop = {
        "ê·¸ë¦¬ê³ ",
        "í•˜ì§€ë§Œ",
        "ê·¼ë°",
        "í˜¹ì‹œ",
        "ë§Œì•½",
        "ë°›ì„",
        "ê°€ëŠ¥",
        "ë¬¸ì˜",
        "ì‹ ì²­",
        "ì—¬ë¶€",
        "ìˆë‚˜ìš”",
        "í•´ë‹¹",
        "ì‚¬ìš©ì",
        "ìƒíƒœ",
        "í˜„ì¬",
        "ì§ˆë¬¸",
        "í˜œíƒ",
        "ì§€ì›",
        "ì •ì±…",
        "ì œê°€",
        "ë‚˜ëŠ”",
        "ì €ëŠ”",
        "ë‚´ê°€",
        "ê¶ê¸ˆ",
        "ê¶ê¸ˆí•´ìš”",
    }
    out: List[str] = []
    seen: set[str] = set()
    for t in tokens:
        t = t.lower()
        if len(t) >= 2 and t not in stop:
            if t not in seen:
                seen.add(t)
                out.append(t)
                if len(out) >= max_k:
                    break
    return out


def _parse_created_at(tri: Dict[str, Any]) -> Optional[datetime]:
    """
    tripleì˜ created_atì„ datetimeìœ¼ë¡œ íŒŒì‹±.
    - ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜.
    (í˜„ì¬ ê³„ì¸µ weight ì¤‘ì‹¬ì´ë¼ í•„ìˆ˜ëŠ” ì•„ë‹˜, í˜¸í™˜ì„±ìš©)
    """
    v = tri.get("created_at")
    if not v:
        return None
    try:
        return datetime.fromisoformat(str(v))
    except Exception:
        return None


# -------------------------------------------------------------------
# BM25 Re-ranking helpers
# -------------------------------------------------------------------
def _tokenize_for_bm25(text: str) -> List[str]:
    """ë‹¨ìˆœ í† í¬ë‚˜ì´ì €: í•œê¸€/ì˜ë¬¸/ìˆ«ì í† í°ì„ ì†Œë¬¸ìë¡œ ë°˜í™˜."""
    if not text:
        return []
    return [t.lower() for t in re.findall(r"[ê°€-í£A-Za-z0-9]+", text)]


def _add_layer_terms(
    terms: List[str],
    layer: Optional[Dict[str, Any]],
    weight: int,
) -> None:
    """
    íŠ¹ì • ì»¬ë ‰ì…˜ ë ˆì´ì–´ì˜ triplesì—ì„œ BM25 termë“¤ì„ ì¶”ì¶œí•˜ì—¬,
    ì£¼ì–´ì§„ weightë§Œí¼ ë°˜ë³µ ì‚½ì….
    """
    if not isinstance(layer, dict):
        return
    triples = layer.get("triples") or []
    if not isinstance(triples, list):
        return

    for tri in triples:
        if not isinstance(tri, dict):
            continue
        obj = (tri.get("object") or "").strip()
        code = (tri.get("code") or "").strip()
        if not obj and not code:
            continue
        toks = _tokenize_for_bm25(f"{obj} {code}")
        if not toks:
            continue

        for tok in toks:
            if not tok:
                continue
            # ê° triple termì€ weight ë²ˆê¹Œì§€ í—ˆìš©
            for _ in range(max(weight, 1)):
                terms.append(tok)


def _build_bm25_terms_from_layers(
    collection_L0: Optional[Dict[str, Any]],
    collection_L1: Optional[Dict[str, Any]],
    collection_L2: Optional[Dict[str, Any]],
) -> List[str]:
    """
    BM25ìš© ì¿¼ë¦¬ í† í° êµ¬ì„±.

    - í˜„ì¬ user_queryëŠ” ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ (ê³„ì¸µ ì„¤ê³„ì— ë§ì¶° ì œê±°).
    - ì»¬ë ‰ì…˜ ê³„ì¸µì„ í†µí•´ "ìƒíƒœ/ì§ˆí™˜/ì¹˜ë£Œ" í‚¤ì›Œë“œë§Œ ë°˜ì˜.

    ê³„ì¸µ êµ¬ì¡°:
      L0: ì´ë²ˆ í„´ì—ì„œ ìƒˆë¡œ ì¶”ì¶œëœ triples (ê°€ì¥ ì¤‘ìš”)
      L1: ì´ë²ˆ ì„¸ì…˜ ephemeral_collection
      L2: DB collections (ê°€ì¥ ë‚®ì€ weight)
    """
    terms: List[str] = []

    _add_layer_terms(terms, collection_L0, LAYER_WEIGHTS.get("L0", 3))
    _add_layer_terms(terms, collection_L1, LAYER_WEIGHTS.get("L1", 2))
    _add_layer_terms(terms, collection_L2, LAYER_WEIGHTS.get("L2", 1))

    return terms


def _apply_bm25_rerank(
    docs: List[Dict[str, Any]],
    query_terms: List[str],
) -> None:
    """ì£¼ì–´ì§„ í›„ë³´ docsì— ëŒ€í•´ BM25 ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  score í•„ë“œë¥¼ hybrid ì ìˆ˜ë¡œ ê°±ì‹ ."""
    if not docs or not query_terms:
        return

    # ë¬¸ì„œë³„ í† í°/ê¸¸ì´/term frequency ê³„ì‚°
    doc_tokens: List[List[str]] = []
    doc_lens: List[int] = []
    term_doc_freq: Dict[str, int] = {t: 0 for t in query_terms}

    for doc in docs:
        text_parts = [
            doc.get("title") or "",
            doc.get("requirements") or "",
            doc.get("benefits") or "",
        ]
        tokens = _tokenize_for_bm25(" ".join(text_parts))
        doc_tokens.append(tokens)
        dl = len(tokens) or 1
        doc_lens.append(dl)

        # ê° ì¿¼ë¦¬ termì´ ë“±ì¥í•˜ëŠ”ì§€ ì„¸ê¸°
        token_set = set(tokens)
        for t in query_terms:
            if t in token_set:
                term_doc_freq[t] = term_doc_freq.get(t, 0) + 1

    N = len(docs)
    avgdl = sum(doc_lens) / float(N)

    # BM25 íŒŒë¼ë¯¸í„°
    k1 = 1.5
    b = 0.75

    bm25_scores: List[float] = []
    for idx, tokens in enumerate(doc_tokens):
        tf: Dict[str, int] = {}
        for tok in tokens:
            if tok in query_terms:
                tf[tok] = tf.get(tok, 0) + 1

        dl = doc_lens[idx]
        score = 0.0
        for term in query_terms:
            n_qi = term_doc_freq.get(term, 0)
            if n_qi == 0:
                continue
            # BM25 idf
            idf = math.log((N - n_qi + 0.5) / (n_qi + 0.5) + 1)
            freq = tf.get(term, 0)
            if freq == 0:
                continue
            denom = freq + k1 * (1 - b + b * dl / avgdl)
            score += idf * (freq * (k1 + 1)) / denom
        bm25_scores.append(score)

    max_bm25 = max(bm25_scores) if bm25_scores else 0.0

    # hybrid ì ìˆ˜ ê³„ì‚°: similarity(ë²¡í„°) + BM25
    for doc, bm25 in zip(docs, bm25_scores):
        # raw similarityëŠ” ë³„ë„ í•„ë“œë¡œ ìœ ì§€
        sim_val = doc.get("similarity")
        try:
            sim = float(sim_val) if sim_val is not None else 0.0
        except (TypeError, ValueError):
            sim = 0.0

        bm25_norm = (bm25 / max_bm25) if max_bm25 > 0 else 0.0
        hybrid = (1.0 - BM25_WEIGHT) * sim + BM25_WEIGHT * bm25_norm

        doc["bm25_score"] = bm25
        # LLM/í›„ì† ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ìµœì¢… scoreë¥¼ hybridë¡œ ë®ì–´ì”€
        doc["score"] = hybrid


# -------------------------------------------------------------------
# Region Sanitizer
# -------------------------------------------------------------------
def _sanitize_region(region_value: Optional[Any]) -> Optional[str]:
    """
    region ê°’ì„ ë¬¸ìì—´ë¡œ ì •ë¦¬.
    - dict í˜•íƒœ({'value': 'ê°•ë‚¨êµ¬'})ë„ ì§€ì›.
    - 'ì„œìš¸ì‹œ ë™ì‘êµ¬', 'ì„œìš¸íŠ¹ë³„ì‹œ ë™ì‘êµ¬' ë“±ì€ ë§ˆì§€ë§‰ í† í°('ë™ì‘êµ¬')ë§Œ ë‚¨ê¹€.
    - ê³µë°±/ë¹ˆ ë¬¸ìì—´ì´ë©´ None.
    """
    if region_value is None:
        return None

    if isinstance(region_value, dict):
        region_value = region_value.get("value")

    if region_value is None:
        return None

    region_str = str(region_value).strip()
    if not region_str:
        return None

    # ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê³ , ë’¤ì—ì„œë¶€í„° 'êµ¬/êµ°/ë™/ì‹œ'ë¡œ ëë‚˜ëŠ” í† í° ì°¾ê¸°
    tokens = region_str.split()
    for tok in reversed(tokens):
        tok = tok.strip()
        if not tok:
            continue
        # 'ë™ì‘êµ¬', 'ë¶„ë‹¹êµ¬', 'ì²­ì£¼ì‹œ' ë“±
        if tok.endswith(("êµ¬", "êµ°", "ë™", "ì‹œ")):
            return tok

    # ìœ„ì—ì„œ ëª» ì°¾ìœ¼ë©´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    return region_str


# -------------------------------------------------------------------
# Synthetic Query Builder
# -------------------------------------------------------------------
def _collect_layer_objects(layer: Optional[Dict[str, Any]]) -> List[str]:
    """
    ì»¬ë ‰ì…˜ ë ˆì´ì–´ì—ì„œ object í…ìŠ¤íŠ¸ë§Œ ëª¨ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    (predicateëŠ” ì—¬ê¸°ì„œ êµ¬ë¶„í•˜ì§€ ì•Šê³ , ì§ˆí™˜/ì¹˜ë£Œ/ì—í”¼ì†Œë“œ ì „ë¶€ ìƒíƒœ ì‹ í˜¸ë¡œ ì‚¬ìš©)
    """
    results: List[str] = []
    if not isinstance(layer, dict):
        return results
    triples = layer.get("triples") or []
    if not isinstance(triples, list):
        return results

    for tri in triples:
        if not isinstance(tri, dict):
            continue
        obj = (tri.get("object") or "").strip()
        if obj:
            results.append(obj)
    return results


def _build_synthetic_query(
    raw_query: str,
    profile_summary_text: Optional[str],
    collection_L0: Optional[Dict[str, Any]],
    collection_L1: Optional[Dict[str, Any]],
) -> str:
    """
    ì§ˆë¬¸ì´ ë„ˆë¬´ ì¼ë°˜ì ì¼ ë•Œ(title ì„ë² ë”©ì— ê·¸ëŒ€ë¡œ ì“°ë©´ ì˜ë¯¸ ì—†ëŠ” ê²½ìš°),
    ì‚¬ìš©ì ìƒíƒœ + ìµœê·¼ ì»¬ë ‰ì…˜ ê¸°ë°˜ìœ¼ë¡œ synthetic queryë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤.

    ê·œì¹™:
      - raw_query ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
      - extract_keywords(raw_query)ê°€ ë¹„ì–´ ìˆìœ¼ë©´ "generic" ìœ¼ë¡œ ë³´ê³  synthetic ì‚¬ìš©.
    """
    raw_query = (raw_query or "").strip()
    core_kws = extract_keywords(raw_query, max_k=4)

    # ì •ë³´ê°€ ìˆëŠ” ì§ˆë¬¸ì´ë©´ ê·¸ëƒ¥ ì›ë¬¸ ì‚¬ìš©
    if core_kws:
        return raw_query

    pieces: List[str] = []

    if profile_summary_text:
        pieces.append(profile_summary_text.strip())

    # ìµœê·¼ ìƒíƒœ/ì§ˆí™˜/ì¹˜ë£Œ í‚¤ì›Œë“œë¥¼ ëª¨ì€ë‹¤ (L0 > L1 ìˆœì„œ)
    objs: List[str] = []
    objs.extend(_collect_layer_objects(collection_L0))
    objs.extend(_collect_layer_objects(collection_L1))

    uniq_objs: List[str] = []
    seen: set[str] = set()
    for o in objs:
        if o not in seen:
            seen.add(o)
            uniq_objs.append(o)
        if len(uniq_objs) >= 5:
            break

    if uniq_objs:
        pieces.append("ìµœê·¼ ìƒí™©: " + ", ".join(uniq_objs))

    # ìƒíƒœ/ì»¬ë ‰ì…˜ ë‘˜ ë‹¤ ë¹„ì–´ìˆìœ¼ë©´ fallbackìœ¼ë¡œ raw_query ì‚¬ìš©
    if not pieces:
        return raw_query

    pieces.append("ê´€ë ¨ ì˜ë£ŒÂ·ë³µì§€ ì§€ì› ì •ì±…")
    synthetic = " ".join(pieces)
    print(f"[policy_retriever_node] synthetic query used instead of raw: {synthetic}")
    return synthetic


# -------------------------------------------------------------------
# Hybrid Document Search (ì œëª© title ì„ë² ë”©ë§Œ ì‚¬ìš©)
# -------------------------------------------------------------------
def _hybrid_search_documents(
    query_text: str,
    merged_profile: Optional[Dict[str, Any]],
    top_k: int = 8,
) -> Tuple[List[Dict[str, Any]], List[str]]:
    """
    query_text ì„ë² ë”© ê¸°ë°˜ pgvector ê²€ìƒ‰.
    - query_text: ì˜¤ì§ "ì •ì±… ìš”ì²­ìš©" í…ìŠ¤íŠ¸ (raw ë˜ëŠ” synthetic)ë§Œ ì‚¬ìš©
    - title ì„ë² ë”©ë§Œ ì‚¬ìš©í•´ì„œ ì •ì±… ì œëª©ê³¼ì˜ ìœ ì‚¬ë„ ì¸¡ì •
    - region ì€ DB ë ˆë²¨ í•˜ë“œ í•„í„°ë§
    """
    query_text = (query_text or "").strip()
    if not query_text:
        return [], []

    # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¡œê·¸ìš©)
    debug_keywords = extract_keywords(query_text, max_k=8)

    # 1) region filter
    region_filter: Optional[str] = None
    if merged_profile:
        region_val = merged_profile.get("residency_sgg_code")
        if region_val is None:
            region_val = merged_profile.get("region_gu")
        print("[policy_retriever_node] merged_profile region_raw:", region_val)
        region_filter = _sanitize_region(region_val)
        print("[policy_retriever_node] region_filter after sanitize:", region_filter)
        if region_filter is None:
            print("[policy_retriever_node] region_filter empty or missing")
    else:
        print("[policy_retriever_node] merged_profile is None or empty")

    # ğŸ” íƒ€ì´ë° ì¸¡ì • ì‹œì‘
    import time

    func_start = time.time()

    # 2) ì„ë² ë”© ê³„ì‚° (ì •ì±… ìš”ì²­ìš© í…ìŠ¤íŠ¸)
    embed_start = time.time()
    try:
        qvec = _embed_text(query_text)
    except Exception as e:
        print(f"[policy_retriever_node] embed failed: {e}")
        return [], debug_keywords
    embed_elapsed = time.time() - embed_start
    print(f"ğŸ” [Embedding] {embed_elapsed:.2f}s", flush=True)

    # psycopg3ì—ì„œ VECTOR íƒ€ì…ìœ¼ë¡œ ìºìŠ¤íŒ…í•˜ê¸° ìœ„í•´ ë¬¸ìì—´ ë¦¬í„°ëŸ´ ì‚¬ìš©
    qvec_str = "[" + ",".join(f"{v:.6f}" for v in qvec) + "]"

    # 3) pgvector ê²€ìƒ‰ + (ì„ íƒì ) ì§€ì—­ í•˜ë“œí•„í„°
    # âœ… region í•„í„°ë¥¼ ë²¡í„° ê²€ìƒ‰ ë‹¨ê³„ì— ì ìš© (í•„í„°ë§ëœ ë¬¸ì„œ ë‚´ì—ì„œë§Œ ê²€ìƒ‰)
    if region_filter:
        sql = """
            SELECT
                d.id,
                d.title,
                d.requirements,
                d.benefits,
                d.region,
                d.url,
                (1 - (e.embedding <=> %(qvec)s::vector)) AS similarity
            FROM embeddings e
            JOIN documents d ON d.id = e.doc_id
            WHERE e.field = 'title'
              AND d.region = %(region)s
            ORDER BY e.embedding <=> %(qvec)s::vector
            LIMIT %(limit)s
        """
        params = {"qvec": qvec_str, "region": region_filter, "limit": top_k}
    else:
        # region í•„í„° ì—†ì„ ë•Œ: ë‹¨ìˆœ ë²¡í„° ê²€ìƒ‰ (ì¸ë±ìŠ¤ ì‚¬ìš©)
        sql = """
            SELECT
                d.id,
                d.title,
                d.requirements,
                d.benefits,
                d.region,
                d.url,
                (1 - (e.embedding <=> %(qvec)s::vector)) AS similarity
            FROM embeddings e
            JOIN documents d ON d.id = e.doc_id
            WHERE e.field = 'title'
            ORDER BY e.embedding <=> %(qvec)s::vector
            LIMIT %(limit)s
        """
        params = {"qvec": qvec_str, "limit": top_k}

    rows = []
    db_start = time.time()
    pool = _get_connection_pool()
    with pool.connection() as conn:
        with conn.cursor() as cur:
            cur.execute(sql, params)
            rows = cur.fetchall()
    db_elapsed = time.time() - db_start

    # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§: 0.1ì´ˆ(100ms) ì´ìƒì´ë©´ ê²½ê³ 
    if db_elapsed > 0.1:
        print(
            f"ğŸ”´ [SLOW DB QUERY] {db_elapsed:.2f}s (expected <0.1s) - Connection pool ìƒíƒœ í™•ì¸ í•„ìš”",
            flush=True,
        )

    print(f"ğŸ” [DB Query] {db_elapsed:.2f}s, returned {len(rows)} rows", flush=True)

    # 4) ê²°ê³¼ ê°€ê³µ â†’ rag_snippets í¬ë§·
    results: List[Dict[str, Any]] = []
    for r in rows:
        similarity = float(r[6]) if r[6] is not None else None
        requirements = (r[2] or "").strip() if isinstance(r[2], str) else None
        benefits = (r[3] or "").strip() if isinstance(r[3], str) else None
        region = (r[4] or "").strip() if isinstance(r[4], str) else None
        url = (r[5] or "").strip() if isinstance(r[5], str) else None

        snippet_lines: List[str] = []
        if requirements:
            snippet_lines.append(f"[ì‹ ì²­ ìš”ê±´]\n{requirements}")
        if benefits:
            snippet_lines.append(f"[ì§€ì› ë‚´ìš©]\n{benefits}")
        snippet_text = "\n\n".join(snippet_lines).strip()

        results.append(
            {
                "doc_id": r[0],
                "title": (r[1] or "").strip() if isinstance(r[1], str) else None,
                "requirements": requirements,
                "benefits": benefits,
                "region": region,
                "url": url,
                "similarity": similarity,
                "snippet": snippet_text,
            }
        )

    # similarity ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (SQLì—ì„œë„ ì •ë ¬í•˜ì§€ë§Œ í˜¹ì‹œ ëª°ë¼ í•œ ë²ˆ ë”)
    results.sort(
        key=lambda x: (x["similarity"] is not None, x["similarity"]), reverse=True
    )

    # rag_snippets í¬ë§·ìœ¼ë¡œ ì¬êµ¬ì„±
    snippets: List[Dict[str, Any]] = []
    for r in results:
        snippet_entry: Dict[str, Any] = {
            "doc_id": r["doc_id"],
            "title": r["title"],
            "source": r["region"] or "policy_db",
            "snippet": r["snippet"] or r["benefits"] or r["requirements"] or "",
            # ì´ˆê¸° scoreëŠ” ë²¡í„° ìœ ì‚¬ë„ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
            "similarity": r["similarity"],
            "score": r["similarity"],
        }
        if r["region"]:
            snippet_entry["region"] = r["region"]
        if r["url"]:
            snippet_entry["url"] = r["url"]
        if r["requirements"]:
            snippet_entry["requirements"] = r["requirements"]
        if r["benefits"]:
            snippet_entry["benefits"] = r["benefits"]
        snippets.append(snippet_entry)

    # ğŸ” íƒ€ì´ë° ì¸¡ì • ì¢…ë£Œ
    func_elapsed = time.time() - func_start
    print(f"ğŸ” [_hybrid_search_documents] Total: {func_elapsed:.2f}s", flush=True)

    return snippets, debug_keywords


# -------------------------------------------------------------------
# use_rag ê²°ì • í•¨ìˆ˜
# -------------------------------------------------------------------
def _decide_use_rag(router: Optional[Dict[str, Any]], query_text: str) -> bool:
    """
    router ì •ë³´ì™€ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ RAG ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •.
    - 1ìˆœìœ„: router["use_rag"] ê°’
    - 2ìˆœìœ„: category/í…ìŠ¤íŠ¸ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±
    """
    if not router:
        return True

    if "use_rag" in router:
        return bool(router["use_rag"])

    text = (query_text or "").lower()
    if any(
        k in text for k in ["ìê²©", "ì§€ì›", "í˜œíƒ", "ëŒ€ìƒ", "ìš”ê±´", "ê¸‰ì—¬", "ë³¸ì¸ë¶€ë‹´"]
    ):
        return True

    return False


# -------------------------------------------------------------------
# ë©”ì¸ ë…¸ë“œ í•¨ìˆ˜
# -------------------------------------------------------------------
@traceable
def policy_retriever_node(state: State) -> State:
    """
    LangGraph ë…¸ë“œ:

    ì…ë ¥:
      - user_context_nodeì—ì„œ ì±„ìš´ ê°’ë“¤:
          * merged_profile / merged_collection
          * collection_layer_L0 / L1 / L2
          * profile_summary_text
          * history_text
          * rolling_summary
      - router: dict (use_rag ë“±)
      - user_input: str (í˜„ì¬ ì§ˆë¬¸)  â† info_extractorê°€ ì •ë³´ë¶€ë¶„ ì œê±°í•œ "ì •ì±… ìš”ì²­ ë¬¸ì¥"ì¼ ìˆ˜ ìˆìŒ

    ì¶œë ¥/ê°±ì‹ :
      - state["retrieval"], state["rag_snippets"], state["context"]
    """
    # raw user query (ì •ë³´ ì œê±° í›„ ì •ì±… ìš”ì²­ ë¬¸ì¥ì¼ ìˆ˜ ìˆìŒ)
    query_text = state.get("user_input") or ""
    router_info: Dict[str, Any] = state.get("router") or {}

    merged_profile: Optional[Dict[str, Any]] = state.get("merged_profile")
    merged_collection: Optional[Dict[str, Any]] = state.get("merged_collection")
    profile_summary_text: Optional[str] = state.get("profile_summary_text")
    history_text: Optional[str] = state.get("history_text")
    rolling_summary: Optional[str] = state.get("rolling_summary")

    # ì»¬ë ‰ì…˜ ê³„ì¸µ ë ˆì´ì–´
    collection_L0: Optional[Dict[str, Any]] = state.get("collection_layer_L0")
    collection_L1: Optional[Dict[str, Any]] = state.get("collection_layer_L1")
    collection_L2: Optional[Dict[str, Any]] = state.get("collection_layer_L2")

    # 1) search_text (ë¡œê·¸/ë””ë²„ê¹…ìš©) - ì„ë² ë”©ì—ëŠ” ì‚¬ìš© ì•ˆ í•¨
    search_parts: List[str] = []

    if profile_summary_text:
        search_parts.append(profile_summary_text)

    # ì»¬ë ‰ì…˜ì˜ ì§ˆí™˜/ì¹˜ë£Œë¥¼ ë¬¸ì¥ìœ¼ë¡œ í’€ì–´ì£¼ê¸° (ì •ë³´ ì œê³µìš©)
    if merged_collection and isinstance(merged_collection, dict):
        diseases: List[str] = []
        treatments: List[str] = []
        for tri in merged_collection.get("triples") or []:
            if tri.get("predicate") == "disease":
                diseases.append(tri.get("object"))
            elif tri.get("predicate") == "treatment":
                treatments.append(tri.get("object"))
        extra_lines: List[str] = []
        if diseases:
            extra_lines.append("ì£¼ìš” ì§ˆí™˜: " + ", ".join(diseases))
        if treatments:
            extra_lines.append("ì£¼ìš” ì¹˜ë£Œ: " + ", ".join(treatments))
        if extra_lines:
            search_parts.append("\n".join(extra_lines))

    if history_text:
        # ì´ì „ ëŒ€í™” í…ìŠ¤íŠ¸ëŠ” ê²€ìƒ‰ì— ì“°ì§€ëŠ” ì•Šì§€ë§Œ, ìš”ì•½/ì„¤ëª…ìš©ìœ¼ë¡œë§Œ í¬í•¨ ê°€ëŠ¥
        search_parts.append("ìµœê·¼ ëŒ€í™” ìš”ì•½:\n" + history_text.strip())

    if query_text:
        search_parts.append("í˜„ì¬ ì§ˆë¬¸: " + query_text.strip())

    search_text = "\n\n".join(search_parts).strip() if search_parts else query_text

    # --- RAG ì‚¬ìš© ì—¬ë¶€ ê²°ì • (raw query ê¸°ì¤€) ---
    use_rag = _decide_use_rag(router_info, query_text)

    rag_docs: List[Dict[str, Any]] = []
    debug_keywords: List[str] = []

    if use_rag and query_text.strip():
        try:
            # 1) synthetic ì—¬ë¶€ íŒë‹¨ + ì •ì±…ìš© embedding query ìƒì„±
            embedding_query = _build_synthetic_query(
                raw_query=query_text,
                profile_summary_text=profile_summary_text,
                collection_L0=collection_L0,
                collection_L1=collection_L1,
            )

            # 2) ê²€ìƒ‰ì—ëŠ” embedding_queryë§Œ ì‚¬ìš©
            rag_docs, debug_keywords = _hybrid_search_documents(
                query_text=embedding_query,
                merged_profile=merged_profile,
                top_k=RAW_TOP_K,
            )
        except Exception as e:  # noqa: E722
            print(f"[policy_retriever_node] document search failed: {e}")
            rag_docs = []
            debug_keywords = extract_keywords(query_text, max_k=8)
    else:
        debug_keywords = extract_keywords(query_text, max_k=8)

    # --- í”„ë¡œí•„ ê¸°ë°˜ í›„ë³´ í•„í„° ì ìš© (ì¤‘ìœ„ì†Œë“/ê¸°ì´ˆìˆ˜ê¸‰/ì¥ì•  ë“± hard filter ì—­í• ) ---
    if merged_profile and rag_docs:
        before = len(rag_docs)
        rag_docs = filter_candidates_by_profile(rag_docs, merged_profile)
        after = len(rag_docs)
        print(f"[policy_retriever_node] profile filter: {before} -> {after} candidates")

    bm25_terms: List[str] = []

    # --- similarity ê¸°ë°˜ ì†Œí”„íŠ¸ ì»·ì˜¤í”„ (ìµœì†Œ ê°œìˆ˜ ë³´ì¥) + BM25 re-ranking ---
    if rag_docs:

        def _get_sim(d: Dict[str, Any]) -> Optional[float]:
            v = d.get("similarity")
            try:
                return float(v) if v is not None else None
            except (TypeError, ValueError):
                return None

        sims = [s for s in (_get_sim(d) for d in rag_docs) if s is not None]
        if sims:
            filtered_by_sim = [
                d for d in rag_docs if (_get_sim(d) or 0.0) >= SIMILARITY_FLOOR
            ]
            if len(filtered_by_sim) >= MIN_CANDIDATES_AFTER_FLOOR:
                print(
                    f"[policy_retriever_node] similarity floor {SIMILARITY_FLOOR}: "
                    f"{len(rag_docs)} -> {len(filtered_by_sim)} candidates"
                )
                rag_docs = filtered_by_sim

        # --- BM25 ê¸°ë°˜ re-ranking (ì»¬ë ‰ì…˜ ê³„ì¸µ ê¸°ë°˜) ---
        bm25_terms = _build_bm25_terms_from_layers(
            collection_L0,
            collection_L1,
            collection_L2,
        )
        if bm25_terms:
            print(f"[policy_retriever_node] BM25 re-ranking with terms: {bm25_terms}")
            _apply_bm25_rerank(rag_docs, bm25_terms)

        # hybrid score(ë²¡í„°+BM25)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (Noneì€ ë’¤ë¡œ)
        def _get_score(d: Dict[str, Any]) -> Optional[float]:
            v = d.get("score")
            try:
                return float(v) if v is not None else None
            except (TypeError, ValueError):
                return None

        rag_docs.sort(
            key=lambda d: (
                _get_score(d) is None,
                -(_get_score(d) or 0.0),
            )
        )

        # LLMì— ë„˜ê¸¸ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ ì œí•œ
        if len(rag_docs) > CONTEXT_TOP_K:
            print(
                f"[policy_retriever_node] context_top_k cap {CONTEXT_TOP_K}: "
                f"{len(rag_docs)} -> {CONTEXT_TOP_K} candidates"
            )
            rag_docs = rag_docs[:CONTEXT_TOP_K]

    # --- ëŒ€í™” ì €ì¥ ì•ˆë‚´ ìŠ¤ë‹ˆí« ì¶”ê°€ ---
    end_requested = bool(state.get("end_session"))
    save_keywords = ("ì €ì¥", "ë³´ê´€", "ê¸°ë¡")
    refers_to_save = any(k in query_text for k in save_keywords)
    if end_requested or refers_to_save:
        rag_docs.append(
            {
                "doc_id": "system:conversation_persist",
                "title": "ëŒ€í™” ì €ì¥ ì•ˆë‚´",
                "snippet": "ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë©´ ì €ì¥ íŒŒì´í”„ë¼ì¸ì´ ìë™ ì‹¤í–‰ë˜ì–´ ëŒ€í™” ë‚´ìš©ì´ ë³´ê´€ë©ë‹ˆë‹¤.",
                "score": 1.0,
            }
        )

    # --- retrieval.keywords êµ¬ì„± ---
    # 1) ì‚¬ìš©ì raw queryì—ì„œ ì˜¨ í‚¤ì›Œë“œ
    user_kw = extract_keywords(query_text, max_k=8)
    # 2) BM25 terms ì™€ í•©ì³ì„œ ì¤‘ë³µ ì œê±°
    final_keywords: List[str] = []
    seen_kw: set[str] = set()
    for t in user_kw + bm25_terms:
        if t not in seen_kw:
            seen_kw.add(t)
            final_keywords.append(t)
            if len(final_keywords) >= 12:
                break

    # --- retrieval ì„¸íŒ… ---
    retrieval: Dict[str, Any] = {
        "used_rag": use_rag,
        "profile_ctx": merged_profile,
        "collection_ctx": merged_collection,
        "rag_snippets": rag_docs,
        "keywords": final_keywords,
        "search_text": search_text,  # ë””ë²„ê¹…/ë¡œê·¸ìš© ì „ì²´ í…ìŠ¤íŠ¸
        "profile_summary_text": profile_summary_text,
    }
    state["retrieval"] = retrieval
    state["rag_snippets"] = rag_docs

    # answer_llmì´ ë°”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” context ë¸”ë¡
    state["context"] = {
        "profile": merged_profile,
        "collection": merged_collection,
        "documents": rag_docs,
        "summary": rolling_summary,
    }

    return state
