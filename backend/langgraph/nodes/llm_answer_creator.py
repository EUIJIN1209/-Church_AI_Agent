# llm_answer_creator.py (Gemini Version)
# ëª©ì : "Answer LLM" ë…¸ë“œ
# - RetrievalPlannerì˜ ê²°ê³¼ë¥¼ ë°›ì•„ ìµœì¢… ë‹µë³€ ìƒì„±
# - Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±

from __future__ import annotations

import json
import os
import time  # ğŸ”¥ ì¶”ê°€: íƒ€ì´ë° ì¸¡ì •ìš©
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
from openai import OpenAI

from app.langgraph.state.ephemeral_context import State as GraphState, Message

load_dotenv()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
ANSWER_MODEL = os.getenv("ANSWER_MODEL", "gpt-4o-mini")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì˜ë£ŒÂ·ë³µì§€ ì§€ì› ì •ì±… ì•ˆë‚´ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

## ì—­í• 
1. ì´ë¯¸ í•„í„°ë§ì´ ì™„ë£Œëœ ì •ì±… í›„ë³´ë“¤ì„ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
2. ì œê³µëœ ëª¨ë“  ì •ì±…ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…

## ì¤‘ìš”: ì¶”ê°€ í•„í„°ë§ ê¸ˆì§€
- ì œê³µëœ ì •ì±… ë¬¸ì„œëŠ” ì´ë¯¸ ì‚¬ìš©ì í”„ë¡œí•„ ê¸°ë°˜ìœ¼ë¡œ í•„í„°ë§ì´ ì™„ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤
- ë‹¹ì‹ ì€ ì¶”ê°€ë¡œ ì •ì±…ì„ ì„ ë³„í•˜ê±°ë‚˜ ì œì™¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
- ì œê³µëœ ëª¨ë“  ì •ì±…ì„ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´í•´ì•¼ í•©ë‹ˆë‹¤

## ë‹µë³€ í˜•ì‹
1. **í•œ ì¤„ ê²°ë¡ ** (êµµê²Œ)
2. ê° ì •ì±…ë§ˆë‹¤:
   - ì •ì±…ëª… + ì§€ì—­
   - ì§€ì› ë‚´ìš© (benefits ê¸°ë°˜)
   - ì§€ì› ìê²© (requirements ê¸°ë°˜)
   - ì‚¬ìš©ìì˜ ì–´ë–¤ ìƒíƒœ/ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ”ì§€ ì„¤ëª…
   - ì‹ ì²­ ë°©ë²• (ë¬¸ì„œì— ìˆëŠ” ê²½ìš°ë§Œ)
   - URL (ê° ì •ì±…ë‹¹ 1íšŒë§Œ ì¶œë ¥)
3. ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´

## ì œì•½
- ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(Profile/Collection/ë¬¸ì„œ)ë§Œ ì‚¬ìš©, ì¶”ì¸¡ ê¸ˆì§€
- ì •ë³´ ë¶€ì¡± ì‹œ "ì¶”ê°€ í™•ì¸ í•„ìš”" ëª…ì‹œ
- ë¯¼ê° ê°œì¸ì •ë³´ ìš”êµ¬ ê¸ˆì§€
- ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ê³  ì •ì±… ì œëª©ê³¼ URL ëª©ë¡ ì •ë¦¬
- **ì œê³µëœ ëª¨ë“  ì •ì±…ì„ ì¶œë ¥** (ì„ì˜ë¡œ ê°œìˆ˜ë¥¼ ì¤„ì´ì§€ ì•ŠìŒ)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¨í…ìŠ¤íŠ¸ ìš”ì•½/ì„œì‹í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _format_profile_ctx(p: Optional[Dict[str, Any]]) -> str:
    if not p or "error" in p:
        return ""
    lines: List[str] = []

    if p.get("summary"):
        lines.append(f"- ìš”ì•½: {p['summary']}")

    if p.get("insurance_type"):
        lines.append(f"- ê±´ë³´ ìê²©: {p['insurance_type']}")

    mir_raw = p.get("median_income_ratio")
    if mir_raw is not None:
        try:
            v = float(mir_raw)
            if v <= 10:
                pct = v * 100.0
            else:
                pct = v
            lines.append(f"- ì¤‘ìœ„ì†Œë“ ë¹„ìœ¨: {pct:.1f}%")
        except:
            lines.append(f"- ì¤‘ìœ„ì†Œë“ ë¹„ìœ¨: {mir_raw}")

    if bb := p.get("basic_benefit_type"):
        lines.append(f"- ê¸°ì´ˆìƒí™œë³´ì¥: {bb}")

    if (dg := p.get("disability_grade")) is not None:
        dg_label = {0: "ë¯¸ë“±ë¡", 1: "ì‹¬í•œ", 2: "ì‹¬í•˜ì§€ì•ŠìŒ"}.get(dg, str(dg))
        lines.append(f"- ì¥ì•  ë“±ê¸‰: {dg_label}")

    if (lt := p.get("ltci_grade")) and lt != "NONE":
        lines.append(f"- ì¥ê¸°ìš”ì–‘ ë“±ê¸‰: {lt}")

    if p.get("pregnant_or_postpartum12m") is True:
        lines.append("- ì„ì‹ /ì¶œì‚° 12ê°œì›” ì´ë‚´")

    return "\n".join(lines)


def _format_collection_ctx(items: Optional[List[Dict[str, Any]]]) -> str:
    if not items:
        return ""
    out = []
    for it in items[:5]:
        if "error" in it:
            continue
        segs = []
        if it.get("predicate"):
            segs.append(f"[{it['predicate']}]")
        if it.get("object"):
            segs.append(it["object"])
        out.append("- " + " ".join(segs))
    return "\n".join(out)


def _format_documents(items: Optional[List[Dict[str, Any]]]) -> str:
    if not items:
        return ""
    out: List[str] = []

    for idx, doc in enumerate(items[:4], start=1):
        if not isinstance(doc, dict):
            continue

        title = doc.get("title") or doc.get("doc_id") or f"ë¬¸ì„œ {idx}"
        source = doc.get("source")
        score = doc.get("score")
        url = doc.get("url")
        snippet = doc.get("snippet") or ""

        header = f"{idx}. {title}"
        if source:
            header += f" ({source})"
        if score:
            header += f" [score={score:.3f}]"

        out.append(f"- {header}")
        out.append(f"  > {snippet.strip()}")

        if url:
            out.append(f"  ì¶œì²˜: {url}")

    return "\n".join(out)


def _build_user_prompt(
    input_text: str,
    used: str,
    profile_ctx: Optional[Dict[str, Any]],
    collection_ctx: Optional[List[Dict[str, Any]]],
    summary: Optional[str] = None,
    documents: Optional[List[Dict[str, Any]]] = None,
) -> str:
    prof_block = _format_profile_ctx(profile_ctx)
    coll_block = _format_collection_ctx(collection_ctx)
    doc_block = _format_documents(documents)
    summary_block = (summary or "").strip()

    lines = [f"ì‚¬ìš©ì ì§ˆë¬¸:\n{input_text.strip()}"]
    lines.append(f"\n[Retrieval ì‚¬ìš©: {used}]")

    if prof_block:
        lines.append("\n[Profile ì»¨í…ìŠ¤íŠ¸]\n" + prof_block)
    if coll_block:
        lines.append("\n[Collection ì»¨í…ìŠ¤íŠ¸]\n" + coll_block)
    if summary_block:
        lines.append("\n[Rolling Summary]\n" + summary_block)
    if doc_block:
        lines.append("\n[RAG ë¬¸ì„œ ìŠ¤ë‹ˆí«]\n" + doc_block)

    lines.append(
        """
ìš”êµ¬ ì¶œë ¥:
- ë§¨ ì•ì— **ê²°ë¡  í•œ ë¬¸ì¥**
- ë‹¤ìŒì— ê·¼ê±°(ìœ„ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì¸ìš©)
- ë§ˆì§€ë§‰ì— ë‹¤ìŒ ë‹¨ê³„(ì¦ë¹™, ì¶”ê°€ í™•ì¸, ì‹ ì²­ ê²½ë¡œ)ë¥¼ ê°„ë‹¨íˆ
- ì¶”ì • ê¸ˆì§€, ì»¨í…ìŠ¤íŠ¸ ë°– ì‚¬ì‹¤ ê¸ˆì§€
"""
    )
    return "\n".join(lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM í˜¸ì¶œ (ì¼ë°˜ ëª¨ë“œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def run_answer_llm(
    input_text: str,
    used: str,
    profile_ctx: Optional[Dict[str, Any]],
    collection_ctx: Optional[List[Dict[str, Any]]],
    summary: Optional[str] = None,
    documents: Optional[List[Dict[str, Any]]] = None,
) -> str:
    # ğŸ”¥ íƒ€ì´ë° ì¸¡ì • ì‹œì‘
    total_start = time.time()

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_start = time.time()
    user_prompt = _build_user_prompt(
        input_text,
        used,
        profile_ctx,
        collection_ctx,
        summary=summary,
        documents=documents,
    )
    prompt_elapsed = time.time() - prompt_start

    # í”„ë¡¬í”„íŠ¸ í¬ê¸° ë¡œê·¸
    prompt_len = len(user_prompt)
    print(
        f"ğŸ“ [LLM] í”„ë¡¬í”„íŠ¸ êµ¬ì„±: {prompt_elapsed:.3f}s, ê¸¸ì´: {prompt_len} chars",
        flush=True,
    )

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt},
    ]

    try:
        # ğŸ”¥ API í˜¸ì¶œ íƒ€ì´ë°
        api_start = time.time()
        resp = client.chat.completions.create(
            model=ANSWER_MODEL,
            messages=messages,
            temperature=0.3,
            timeout=60.0,  # íƒ€ì„ì•„ì›ƒ 60ì´ˆë¡œ ì¦ê°€
        )
        api_elapsed = time.time() - api_start

        result = resp.choices[0].message.content.strip()
        result_len = len(result)

        # ğŸ”¥ ì´ ì†Œìš” ì‹œê°„ ë¡œê·¸
        total_elapsed = time.time() - total_start
        print(
            f"âœ… [LLM] API í˜¸ì¶œ: {api_elapsed:.2f}s, ì‘ë‹µ ê¸¸ì´: {result_len} chars",
            flush=True,
        )
        print(f"âœ… [LLM] ì´ ì†Œìš”ì‹œê°„: {total_elapsed:.2f}s", flush=True)

        return result

    except Exception as e:
        total_elapsed = time.time() - total_start
        print(f"ğŸ”¥ [LLM ERROR] {total_elapsed:.2f}s í›„ ì—ëŸ¬ ë°œìƒ: {e}", flush=True)
        raise


def run_answer_llm_stream(
    input_text: str,
    used: str,
    profile_ctx: Optional[Dict[str, Any]],
    collection_ctx: Optional[List[Dict[str, Any]]],
    summary: Optional[str] = None,
    documents: Optional[List[Dict[str, Any]]] = None,
):
    """
    ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ LLM ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ğŸ”¥ íƒ€ì´ë° ì¸¡ì • ì‹œì‘
    total_start = time.time()

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_start = time.time()
    user_prompt = _build_user_prompt(
        input_text,
        used,
        profile_ctx,
        collection_ctx,
        summary=summary,
        documents=documents,
    )
    prompt_elapsed = time.time() - prompt_start
    prompt_len = len(user_prompt)
    print(
        f"ğŸ“ [LLM Stream] í”„ë¡¬í”„íŠ¸ êµ¬ì„±: {prompt_elapsed:.3f}s, ê¸¸ì´: {prompt_len} chars",
        flush=True,
    )

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt},
    ]

    try:
        # ğŸ”¥ API í˜¸ì¶œ íƒ€ì´ë°
        api_start = time.time()
        first_chunk_time = None
        chunk_count = 0
        total_chars = 0

        stream = client.chat.completions.create(
            model=ANSWER_MODEL,
            messages=messages,
            temperature=0.3,
            timeout=60.0,  # íƒ€ì„ì•„ì›ƒ 60ì´ˆë¡œ ì¦ê°€
            stream=True,
        )

        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                chunk_count += 1
                total_chars += len(content)

                # ğŸ”¥ ì²« ë²ˆì§¸ ì²­í¬ ì‹œê°„ ê¸°ë¡
                if first_chunk_time is None:
                    first_chunk_time = time.time() - api_start
                    print(
                        f"âš¡ [LLM Stream] ì²« í† í° ë„ì°©: {first_chunk_time:.2f}s",
                        flush=True,
                    )

                yield content

        # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ë¡œê·¸
        total_elapsed = time.time() - total_start
        api_elapsed = time.time() - api_start
        print(
            f"âœ… [LLM Stream] ì™„ë£Œ: API {api_elapsed:.2f}s, ì´ {total_elapsed:.2f}s, {chunk_count} chunks, {total_chars} chars",
            flush=True,
        )

    except Exception as e:
        total_elapsed = time.time() - total_start
        print(f"ğŸ”¥ [LLM Stream ERROR] {total_elapsed:.2f}s í›„ ì—ëŸ¬: {e}", flush=True)
        yield f"\n\n[ì˜¤ë¥˜ ë°œìƒ: {str(e)}]"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _extract_context_from_messages(messages: List[Message]) -> Dict[str, Any]:
    for msg in reversed(messages or []):
        if msg.get("role") != "tool":
            continue
        if msg.get("content") != "[context_assembler] prompt_ready":
            continue
        meta = msg.get("meta") or {}
        ctx = meta.get("context")
        if isinstance(ctx, dict):
            return ctx
    return {}


def _last_user_content(messages: List[Message]) -> str:
    for msg in reversed(messages or []):
        if msg.get("role") == "user":
            return msg.get("content", "")
    return ""


def _infer_used_flag(profile_ctx: Any, collection_ctx: Any, documents: Any) -> str:
    has_profile = isinstance(profile_ctx, dict) and bool(profile_ctx)
    has_collection = isinstance(collection_ctx, list) and bool(collection_ctx)
    has_docs = isinstance(documents, list) and bool(documents)
    if has_profile and (has_collection or has_docs):
        return "BOTH"
    if has_profile:
        return "PROFILE"
    if has_collection or has_docs:
        return "COLLECTION"
    return "NONE"


def _safe_json(value: Any, limit: int = 400) -> str:
    if not value:
        return "ì—†ìŒ"
    try:
        text = json.dumps(value, ensure_ascii=False)
    except Exception:
        text = str(value)
    return text[:limit] + ("..." if len(text) > limit else "")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Fallback ë©”ì‹œì§€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _build_fallback_text(
    used: str,
    profile_ctx: Any,
    collection_ctx: Any,
    documents: Any,
    summary: Optional[str],
) -> str:
    return (
        "ì£„ì†¡í•´ìš”. ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”.\n\n"
        "## ê·¼ê±°(ìš”ì•½)\n"
        f"- Retrieval ì‚¬ìš©: {used}\n"
        f"- Summary: {(summary or 'ì—†ìŒ')[:400]}\n"
        f"- Profile: {_safe_json(profile_ctx)}\n"
        f"- Collection: {_safe_json(collection_ctx)}\n"
        f"- Documents: {_safe_json(documents)}\n"
        "í•„ìš” ì‹œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ answer ë…¸ë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def answer(state: GraphState) -> Dict[str, Any]:
    # ğŸ”¥ ë…¸ë“œ ì „ì²´ íƒ€ì´ë°
    node_start = time.time()
    print(f"ğŸš€ [answer_llm ë…¸ë“œ] ì‹œì‘", flush=True)

    messages: List[Message] = list(state.get("messages") or [])
    retrieval = state.get("retrieval") or {}
    ctx = _extract_context_from_messages(messages)

    profile_ctx = ctx.get("profile") or retrieval.get("profile_ctx")
    collection_ctx = ctx.get("collection") or retrieval.get("collection_ctx")

    if isinstance(collection_ctx, dict) and "triples" in collection_ctx:
        collection_ctx_list = collection_ctx["triples"]
    elif isinstance(collection_ctx, list):
        collection_ctx_list = collection_ctx
    else:
        collection_ctx_list = None

    documents = ctx.get("documents") or retrieval.get("rag_snippets")
    summary = ctx.get("summary") or state.get("rolling_summary")

    input_text = (
        state.get("user_input") or state.get("input_text") or ""
    ).strip() or _last_user_content(messages).strip()

    used = (retrieval.get("used") or "").strip().upper()
    if not used:
        used = _infer_used_flag(profile_ctx, collection_ctx_list, documents)

    streaming_mode = state.get("streaming_mode", False)

    if streaming_mode:
        text = ""
        print(f"â­ï¸  [answer_llm ë…¸ë“œ] ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ - LLM í˜¸ì¶œ ìŠ¤í‚µ", flush=True)
    else:
        try:
            text = run_answer_llm(
                input_text,
                used,
                profile_ctx,
                collection_ctx_list,
                summary=summary,
                documents=documents,
            )
        except Exception:
            text = _build_fallback_text(
                used,
                profile_ctx,
                collection_ctx_list,
                documents,
                summary,
            )

    citations = {
        "profile": profile_ctx,
        "collection": collection_ctx_list,
        "documents": documents,
    }

    assistant_message: Message = {
        "role": "assistant",
        "content": text,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "meta": {
            "model": ANSWER_MODEL,
            "used": used,
            "citations": {
                "profile": bool(profile_ctx),
                "collection_count": len(collection_ctx_list or []),
                "document_count": len(documents or []),
            },
        },
    }

    # ğŸ”¥ ë…¸ë“œ ì™„ë£Œ ë¡œê·¸
    node_elapsed = time.time() - node_start
    print(f"âœ… [answer_llm ë…¸ë“œ] ì™„ë£Œ: {node_elapsed:.2f}s", flush=True)

    return {
        "answer": {
            "text": text,
            "citations": citations,
            "used": used,
        },
        "messages": [assistant_message],
        "streaming_context": {
            "input_text": input_text,
            "used": used,
            "profile_ctx": profile_ctx,
            "collection_ctx": collection_ctx_list,
            "summary": summary,
            "documents": documents,
        },
    }


def answer_llm_node(state: GraphState) -> Dict[str, Any]:
    return answer(state)
